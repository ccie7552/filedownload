import requests
from bs4 import BeautifulSoup
import time
import re
import os
from urllib.parse import urljoin, parse_qs, urlparse

class IntegratedNedrugScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://nedrug.mfds.go.kr/CCBAR01F012/getList"

    # ==================== 1ë‹¨ê³„: URL ìˆ˜ì§‘ ====================
    
    def get_total_info(self):
        """ì „ì²´ í˜ì´ì§€ ìˆ˜ì™€ ì˜ˆìƒ í•­ëª© ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ íŒŒì•…"""
        print("ğŸ“Š ì „ì²´ í•­ëª© ìˆ˜ ë° í˜ì´ì§€ ìˆ˜ íŒŒì•… ì¤‘...")
        
        try:
            # ì²« í˜ì´ì§€ ìš”ì²­
            response = self.session.get(self.base_url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë°©ë²• 1: ë§ˆì§€ë§‰ í˜ì´ì§€ ë²„íŠ¼ì„ í´ë¦­í–ˆì„ ë•Œì˜ URL íŒŒì•…
            test_response = self.session.get(self.base_url, params={'page': 999}, timeout=15)
            
            if '?totalPages=' in test_response.url:
                # URLì—ì„œ totalPages ì¶”ì¶œ
                parsed_url = urlparse(test_response.url)
                query_params = parse_qs(parsed_url.query)
                total_pages = int(query_params.get('totalPages', [0])[0])
                
                if total_pages > 0:
                    print(f"âœ… URLì—ì„œ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸: {total_pages}í˜ì´ì§€")
                    return total_pages, total_pages * 10  # í˜ì´ì§€ë‹¹ 10ê°œë¡œ ì¶”ì •
            
            # ë°©ë²• 2: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
            pagination_links = soup.find_all('a', href=lambda x: x and '#list' in x)
            max_page = 1
            
            for link in pagination_links:
                text = link.get_text().strip()
                if text.isdigit():
                    max_page = max(max_page, int(text))
            
            print(f"ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€: {max_page}")
            return None, None  # ìˆœì°¨ì  íƒìƒ‰ìœ¼ë¡œ ì „í™˜
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ì •ë³´ íŒŒì•… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, None

    def get_page_data(self, page_num=1):
        """íŠ¹ì • í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        try:
            params = {
                'page': page_num,
                'limit': 10
            }
            
            response = self.session.get(self.base_url, params=params, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
            
        except requests.RequestException as e:
            print(f"âŒ í˜ì´ì§€ {page_num} ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def extract_links_from_html(self, html_content, page_num):
        """HTMLì—ì„œ ì œëª© ë§í¬ë“¤ì„ ì¶”ì¶œ"""
        if not html_content:
            return []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            base_url = "https://nedrug.mfds.go.kr"
            
            table = soup.find('table')
            if not table:
                return []
            
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
            else:
                all_rows = table.find_all('tr')
                rows = all_rows[2:] if len(all_rows) > 2 else []
            
            for idx, row in enumerate(rows):
                cells = row.find_all('td')
                if len(cells) >= 2:
                    title_cell = cells[1]
                    link_tag = title_cell.find('a')
                    
                    if link_tag and link_tag.get('href'):
                        title = link_tag.get_text().strip()
                        href = link_tag.get('href')
                        
                        if href.startswith('/'):
                            full_url = base_url + href
                        else:
                            full_url = urljoin(base_url, href)
                        
                        sequence_num = cells[0].get_text().strip()
                        
                        links.append({
                            'sequence': sequence_num,
                            'title': title,
                            'url': full_url,
                            'page': page_num
                        })
            
            return links
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_num} íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def collect_all_urls(self):
        """ëª¨ë“  í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘"""
        print("ğŸ” ìµœì‹  URL ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("=" * 80)
        
        # ì „ì²´ í˜ì´ì§€ ìˆ˜ íŒŒì•… ì‹œë„
        total_pages, estimated_items = self.get_total_info()
        
        all_links = []
        failed_pages = []
        
        if total_pages and estimated_items:
            # ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ì•„ëŠ” ê²½ìš°
            print(f"ğŸ“Š ì´ í˜ì´ì§€ ìˆ˜: {total_pages}í˜ì´ì§€")
            print(f"ğŸ“ˆ ì˜ˆìƒ í•­ëª© ìˆ˜: {estimated_items}ê°œ")
            print("=" * 40)
            
            for page_num in range(1, total_pages + 1):
                print(f"ğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì¤‘...")
                
                html_content = self.get_page_data(page_num)
                if html_content:
                    page_links = self.extract_links_from_html(html_content, page_num)
                    if page_links:
                        all_links.extend(page_links)
                        print(f"   âœ… {len(page_links)}ê°œ ë§í¬ ìˆ˜ì§‘")
                    else:
                        failed_pages.append(page_num)
                        print(f"   âš ï¸ ë¹ˆ í˜ì´ì§€")
                else:
                    failed_pages.append(page_num)
                    print(f"   âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
                
                time.sleep(1)
                
                if page_num % 10 == 0:
                    print(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(all_links)}ê°œ")
        else:
            # ìˆœì°¨ì  íƒìƒ‰
            print("ğŸ” ìˆœì°¨ì  íƒìƒ‰ ëª¨ë“œë¡œ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            print("âš ï¸ ë¹ˆ í˜ì´ì§€ê°€ 3íšŒ ì—°ì† ë‚˜ì˜¬ ë•Œê¹Œì§€ ê³„ì† íƒìƒ‰í•©ë‹ˆë‹¤.")
            print("=" * 40)
            
            page_num = 1
            consecutive_empty_pages = 0
            max_empty_pages = 3
            
            while consecutive_empty_pages < max_empty_pages:
                print(f"ğŸ“„ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
                
                html_content = self.get_page_data(page_num)
                if html_content:
                    page_links = self.extract_links_from_html(html_content, page_num)
                    if page_links:
                        all_links.extend(page_links)
                        consecutive_empty_pages = 0
                        print(f"   âœ… {len(page_links)}ê°œ ë§í¬ ìˆ˜ì§‘")
                    else:
                        consecutive_empty_pages += 1
                        print(f"   âš ï¸ ë¹ˆ í˜ì´ì§€ ê°ì§€ ({consecutive_empty_pages}/{max_empty_pages})")
                else:
                    consecutive_empty_pages += 1
                    print(f"   âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ({consecutive_empty_pages}/{max_empty_pages})")
                
                time.sleep(1)
                
                if page_num % 10 == 0:
                    print(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(all_links)}ê°œ")
                
                page_num += 1
                
                if page_num > 200:  # ë¬´í•œë£¨í”„ ë°©ì§€
                    print("âš ï¸ ìµœëŒ€ í˜ì´ì§€ ìˆ˜(200) ë„ë‹¬. ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
        
        print("=" * 80)
        print(f"ğŸ‰ URL ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_links)}ê°œì˜ ë§í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        if failed_pages:
            print(f"âš ï¸ ì‹¤íŒ¨í•œ í˜ì´ì§€: {len(failed_pages)}ê°œ")
        
        return all_links

    # ==================== 2ë‹¨ê³„: ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ====================
    
    def extract_detail_content(self, html_content, url):
        """ìƒì„¸ì •ë³´ ë‚´ìš© ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        result = {
            'url': url,
            'title': '',
            'detail_content': ''
        }
        
        try:
            # 1. ì œëª© ì¶”ì¶œ
            basic_info_table = soup.find('table')
            if basic_info_table:
                rows = basic_info_table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        header = cells[0].get_text(strip=True)
                        if header == 'ì œëª©':
                            result['title'] = cells[1].get_text(strip=True)
                            break
            
            # 2. ìƒì„¸ì •ë³´ ë‚´ìš© ì¶”ì¶œ
            textbox = soup.find('textbox')
            if textbox and textbox.get_text(strip=True):
                result['detail_content'] = textbox.get_text(strip=True)
            else:
                # ìƒì„¸ì •ë³´ í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ
                detail_tables = soup.find_all('table')
                for table in detail_tables:
                    rows = table.find_all('tr')
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            header = cells[0].get_text(strip=True)
                            if header == 'ë‚´ìš©':
                                content_cell = cells[1]
                                inner_textbox = content_cell.find('textbox')
                                if inner_textbox:
                                    result['detail_content'] = inner_textbox.get_text(strip=True)
                                else:
                                    result['detail_content'] = content_cell.get_text(strip=True)
                                break
                    if result['detail_content']:
                        break
                        
        except Exception as e:
            print(f"âŒ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
            
        return result

    def get_page_content(self, url):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ({url}): {e}")
            return None

    def extract_details_from_urls(self, url_list, delay=1.5):
        """URL ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ"""
        print(f"\nğŸ” ìƒì„¸ ë‚´ìš© ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print(f"ğŸ“Š ì´ {len(url_list)}ê°œ URL ì²˜ë¦¬ ì˜ˆì •")
        print(f"â° ì˜ˆìƒ ì†Œìš” ì‹œê°„: {len(url_list) * delay / 60:.1f}ë¶„")
        print("=" * 80)
        
        all_data = []
        failed_urls = []
        
        for i, link_info in enumerate(url_list, 1):
            url = link_info['url']
            title = link_info['title']
            
            print(f"ğŸ“‹ ì²˜ë¦¬ ì¤‘... ({i:4d}/{len(url_list)}) {title[:45]}...")
            
            html_content = self.get_page_content(url)
            if html_content:
                detail_info = self.extract_detail_content(html_content, url)
                if detail_info['detail_content'] or detail_info['title']:
                    detail_info['original_title'] = title
                    detail_info['sequence'] = link_info['sequence']
                    all_data.append(detail_info)
                    print(f"     âœ… ì™„ë£Œ")
                else:
                    print(f"     âš ï¸ ë‚´ìš© ì—†ìŒ")
                    failed_urls.append(url)
            else:
                print(f"     âŒ ì‹¤íŒ¨")
                failed_urls.append(url)
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
            if i < len(url_list):
                time.sleep(delay)
            
            # ì§„í–‰ ìƒí™© ì¤‘ê°„ ë³´ê³ 
            if i % 50 == 0:
                success_rate = len(all_data) / i * 100
                remaining_time = (len(url_list) - i) * delay / 60
                print(f"\nğŸ“Š ì¤‘ê°„ ì§„í–‰ ìƒí™©:")
                print(f"   ì§„í–‰ë¥ : {i}/{len(url_list)} ({i/len(url_list)*100:.1f}%)")
                print(f"   ì„±ê³µ: {len(all_data)}ê°œ, ì‹¤íŒ¨: {len(failed_urls)}ê°œ")
                print(f"   ì„±ê³µë¥ : {success_rate:.1f}%")
                print(f"   ë‚¨ì€ ì‹œê°„: ì•½ {remaining_time:.1f}ë¶„")
                print("-" * 80)
        
        print("\n" + "=" * 80)
        print(f"ğŸ‰ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ!")
        print(f"   âœ… ì„±ê³µ: {len(all_data)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")
        print(f"   ğŸ“ˆ ì„±ê³µë¥ : {len(all_data)/(len(url_list))*100:.1f}%")
        print("=" * 80)
        
        return all_data, failed_urls

    # ==================== 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ====================
    
    def save_to_file(self, data_list, filename="detail_context.txt"):
        """ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ìˆœë²ˆìœ¼ë¡œ ì •ë ¬
            try:
                data_sorted = sorted(data_list, key=lambda x: int(x['sequence']) if x['sequence'].isdigit() else 0)
            except:
                data_sorted = data_list
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ ë³€ê²½ëª…ë ¹ ìƒì„¸ì •ë³´\n")
                f.write(f"ìˆ˜ì§‘ ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 80 + "\n\n")
                
                for i, data in enumerate(data_sorted, 1):
                    title = data.get('original_title') or data.get('title', 'ì œëª© ì—†ìŒ')
                    
                    f.write(f"{data.get('sequence', i)}. {title}\n")
                    f.write(f"URL: {data['url']}\n")
                    f.write("-" * 80 + "\n")
                    
                    if data['detail_content']:
                        f.write(f"{data['detail_content']}\n")
                    else:
                        f.write("ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
                    
                    f.write("\n" + "=" * 80 + "\n\n")
                    
            print(f"ğŸ’¾ ìƒì„¸ ë‚´ìš©ì´ {filename} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def save_urls_to_file(self, links, filename='nedrug_links.txt'):
        """URL ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ë°±ì—…ìš©)"""
        try:
            # ìˆœë²ˆìœ¼ë¡œ ì •ë ¬
            try:
                links_sorted = sorted(links, key=lambda x: int(x['sequence']) if x['sequence'].isdigit() else 0)
            except:
                links_sorted = links
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ ë³€ê²½ëª…ë ¹ ë§í¬ ëª©ë¡\n")
                f.write(f"ìˆ˜ì§‘ ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 50 + "\n\n")
                
                for link in links_sorted:
                    f.write(f"{link['sequence']}. {link['title']}\n")
                    f.write(f"{link['url']}\n")
                    f.write("-" * 50 + "\n")
            
            print(f"ğŸ’¾ URL ëª©ë¡ì´ {filename} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ URL íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def save_failed_urls(self, failed_urls, filename="failed_urls.txt"):
        """ì‹¤íŒ¨í•œ URLë“¤ ì €ì¥"""
        if failed_urls:
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write("ì²˜ë¦¬ ì‹¤íŒ¨í•œ URL ëª©ë¡:\n")
                    f.write(f"ìƒì„± ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write("=" * 50 + "\n\n")
                    for url in failed_urls:
                        f.write(f"{url}\n")
                print(f"ğŸ“ ì‹¤íŒ¨í•œ URL ëª©ë¡ì´ {filename} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨ URL íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ==================== ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ ====================
    
    def run_complete_process(self, detail_delay=1.5):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ - í•­ìƒ ìµœì‹  URLë¶€í„° ìˆ˜ì§‘"""
        print("=" * 80)
        print("ğŸš€ ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ í†µí•© ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤")
        print("ğŸ“… ë§¤ì¼ ì—…ë°ì´íŠ¸ë˜ëŠ” ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
        print("=" * 80)
        
        try:
            # 1ë‹¨ê³„: ìµœì‹  URL ìˆ˜ì§‘
            print("\n[1ë‹¨ê³„] ìµœì‹  URL ìˆ˜ì§‘ ì¤‘...")
            all_links = self.collect_all_urls()
            
            if not all_links:
                print("âŒ ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            
            # URL ëª©ë¡ ë°±ì—… ì €ì¥
            self.save_urls_to_file(all_links)
            
            # 2ë‹¨ê³„: ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ë©”ì¸ ì‘ì—…)
            print(f"\n[2ë‹¨ê³„] ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
            detail_data, failed_urls = self.extract_details_from_urls(all_links, detail_delay)
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
            print(f"\n[3ë‹¨ê³„] ê²°ê³¼ ì €ì¥ ì¤‘...")
            if detail_data:
                self.save_to_file(detail_data)
            
            if failed_urls:
                self.save_failed_urls(failed_urls)
            
            # ìµœì¢… ê²°ê³¼ ë³´ê³ 
            print("\n" + "=" * 80)
            print("ğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            print("=" * 80)
            print(f"ğŸ“Š ì „ì²´ URL ìˆ˜: {len(all_links)}ê°œ")
            print(f"âœ… ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {len(detail_data)}ê°œ")
            print(f"âŒ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")
            print(f"ğŸ“ˆ ì„±ê³µë¥ : {len(detail_data)/(len(all_links))*100:.1f}%")
            print("=" * 80)
            print("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
            print("   - detail_context.txt: ìƒì„¸ ë‚´ìš© (ë©”ì¸ ê²°ê³¼)")
            print("   - nedrug_links.txt: URL ëª©ë¡ (ë°±ì—…)")
            if failed_urls:
                print("   - failed_urls.txt: ì‹¤íŒ¨í•œ URL ëª©ë¡")
            print("=" * 80)
            
            return detail_data
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ í†µí•© ìŠ¤í¬ë˜í¼")
    print("âš¡ í•­ìƒ ìµœì‹  URLë¶€í„° ìˆ˜ì§‘í•˜ì—¬ ë‹¹ì¼ ì—…ë°ì´íŠ¸ëœ ì •ë³´ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.")
    
    scraper = IntegratedNedrugScraper()
    
    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (í•­ìƒ ìƒˆë¡œìš´ URL ìˆ˜ì§‘ë¶€í„° ì‹œì‘)
    data = scraper.run_complete_process(detail_delay=1.5)
    
    if data:
        print(f"\nğŸŠ ì´ {len(data)}ê°œì˜ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“‹ detail_context.txt íŒŒì¼ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
